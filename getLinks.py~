# -*- coding:utf-8 -*-
from urllib.request import urlopen
from bs4 import BeautifulSoup
import time

#所需变量初始化及准备工作
#记录程序开始时间
start = time.clock()
#设置网站首页地址
rooturl='http://www.jingpinke.com/'
#创建一个list用于保存课程大类的链接地址
indexlist=[]

##====================以下为方法========================##

'''
方法名：getContent
作用：从pagelist中的url中解析出相关内容，并保存到本地文本中
参数：含有具体课程内容的网址 
'''
def getContent(url):
  try:
    currentPage=urlopen(url).read() #读取源码
  except Exception as err:
    print('联网超时，退出当前页面。')
  currentText=BeautifulSoup(currentPage) #利用bs进行解析

  #获取课程标题、时间、教师等信息，并将它们以空格连接起来作为文件名使用
  title=currentText.find('div',{'class':'cTitle'}).find('h2').get_text()
  teacher=currentText.find('div',{'class':'course_final_ohter'}).findAll('span')[0].get_text()
  university=currentText.find('div',{'class':'course_final_ohter'}).findAll('span')[1].get_text()
  date=currentText.find('div',{'class':'course_final_ohter'}).findAll('span')[2].get_text()
  rank=currentText.find('div',{'class':'course_final_ohter'}).findAll('span')[3].get_text()
  classtitle=date+' '+university+' '+rank+' '+title+' '+teacher

  
  #获取课程简介内容
  briefintro=currentText.find('pre').get_text()

  
  #以标题为文件名，创建txt文件，并写入正文内容
  f=open('file/'+classtitle+'.txt','w+', encoding='utf-8')
  f.write(classtitle+'\r\n'+briefintro)
  print(classtitle+'.txt')
  f.close()
  

#开始爬取
# 1、先解析根目录，获取13个目标链接
print('解析根目录')
rawtext=urlopen(rooturl).read()
soup = BeautifulSoup(rawtext)
targetDiv=soup.find('div',{'class':'benke_fenlei'})
catalogLinks=targetDiv.findAll('a')
for l in catalogLinks:
  indexlist.append(l.get('href'))
print('根目录解析完成')

#2、从indexlist中逐个读取地址，获取该课程大类下的每个课程的地址
for index in indexlist:
  currentpage=index  #初始时，将某大类的链接设为当前页
  count=1 # 计数器，计算某大类下的课程数目
  while True:
    print("当前页："+currentpage)
    try:
      rawtext=urlopen(currentpage).read()  #读取当前页
    except Exception as err:
      print('联网超时，退出当前大类。')
      break
    soup= BeautifulSoup(rawtext) #解析当前页
    eachclass=soup.find('div',{'class':'course_list'}).findAll('table') #获取课程名列表，每一个课程名都放在一个table中
    for c in eachclass: #循环获取每一个课程的链接地址
      print(str(count)+'：http://course.jingpinke.com'+c.find('a').get('href'))  #打印该地址
      count=count+1 # 计数器自增
      getContent('http://course.jingpinke.com'+c.find('a').get('href')) #调用getContent方法，获取该页内容，并保存
    if soup.find(id='nextPage')==None: # 如果当前页未发现下一页链接 说明该大类已经获取完毕
      print('没有了。') # 打印结束
      break #退出外层循环，继续下一大类
    else: #否则，说明还有下一页
      nextpageurl='http://course.jingpinke.com/search'+soup.find(id='nextPage').find('a').get('href') # 获取当前页中的下一页链接
      currentpage=nextpageurl #将当前页中的下一页链接设为当前页
      print('下一页：'+nextpageurl) # 打印下一步要处理的地址
    
#4、计算程序执行过程耗时
end = time.clock()
print (end-start)

